{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renuchaurasia/renu_INFO5731_Spring2023/blob/main/In_class_exercise/In_class_exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu2E1YFvGm9C"
      },
      "source": [
        "# **The fifth in-class-exercise (40 points in total, 4/18/2023)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFr3hkYQGm9D"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text classification as well as the performance evaluation. In addition, you are requried to conduct *10 fold cross validation (https://scikit-learn.org/stable/modules/cross_validation.html)* in the training. \n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data. \n",
        "\n",
        "Algorithms:\n",
        "\n",
        "(1) MultinominalNB\n",
        "\n",
        "(2) SVM \n",
        "\n",
        "(3) KNN \n",
        "\n",
        "(4) Decision tree\n",
        "\n",
        "(5) Random Forest\n",
        "\n",
        "(6) XGBoost\n",
        "\n",
        "(7) Word2Vec\n",
        "\n",
        "(8) BERT\n",
        "\n",
        "Evaluation measurement:\n",
        "\n",
        "(1) Accuracy\n",
        "\n",
        "(2) Recall\n",
        "\n",
        "(3) Precison \n",
        "\n",
        "(4) F-1 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh5uSqUhGm9E",
        "outputId": "070da680-01c8-4ba3-91ec-d252d4d9f2c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Sentiment                                             Review Comment\n",
            "0             0  apparently reassembled from the cutting-room f...     NaN\n",
            "1             0  they presume their audience wo n't sit still f...     NaN\n",
            "2             1  this is a visually stunning rumination on love...     NaN\n",
            "3             1  jonathan parker 's bartleby should have been t...     NaN\n",
            "4             1  campanella gets the tone just right -- funny i...     NaN\n",
            "...         ...                                                ...     ...\n",
            "6914          1  painful , horrifying and oppressively tragic ,...     NaN\n",
            "6915          0  take care is nicely performed by a quintet of ...     NaN\n",
            "6916          0  the script covers huge , heavy topics in a bla...     NaN\n",
            "6917          0  a seriously bad film with seriously warped log...     NaN\n",
            "6918          1  a deliciously nonsensical comedy about a city ...     NaN\n",
            "\n",
            "[6919 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# Read Text Files with Pandas using read_csv()\n",
        "\n",
        "# importing pandas\n",
        "import pandas as pd\n",
        "\n",
        "# read text file into pandas DataFrame\n",
        "df_train = pd.read_fwf(\"C:\\\\Users\\\\RenuChaurasia\\\\OneDrive - UNT System\\\\LeetCode\\\\program\\\\src\\\\python\\\\info5731\\\\exercise05_datacollection\\\\exercise09_datacollection\\\\stsa-train.txt\", sep=\" \")\n",
        "df_train.columns = [\"Sentiment\", \"Review\", \"Comment\"]\n",
        "# display DataFrame\n",
        "print(df_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v43R9cGGm9F",
        "outputId": "54e8884f-9a0a-43e5-dabf-098b375d3668"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Review</th>\n",
              "      <th>Comments</th>\n",
              "      <th>Comments2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>a gob of drivel so sickly sweet , even the eag...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>gangs of new york is an unapologetic mess , wh...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>we never really feel involved with the story ,...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>this is one of polanski 's best films .</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>take care of my cat offers a refreshingly diff...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentiment                                             Review Comments  \\\n",
              "0          0  a gob of drivel so sickly sweet , even the eag...      NaN   \n",
              "1          0  gangs of new york is an unapologetic mess , wh...      NaN   \n",
              "2          0  we never really feel involved with the story ,...      NaN   \n",
              "3          1            this is one of polanski 's best films .      NaN   \n",
              "4          1  take care of my cat offers a refreshingly diff...      NaN   \n",
              "\n",
              "  Comments2  \n",
              "0       NaN  \n",
              "1       NaN  \n",
              "2       NaN  \n",
              "3       NaN  \n",
              "4       NaN  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# read text file into pandas DataFrame\n",
        "df_test = pd.read_fwf(\"C:\\\\Users\\\\RenuChaurasia\\\\OneDrive - UNT System\\\\LeetCode\\\\program\\\\src\\\\python\\\\info5731\\\\exercise05_datacollection\\\\exercise09_datacollection\\\\stsa-test.txt\", sep=\" \")\n",
        "df_test.columns = [\"Sentiment\", \"Review\", \"Comments\",\"Comments2\"]\n",
        "# display DataFrame\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z34MlMMUGm9F"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "X_trainval = df_train['Review']\n",
        "y_trainval = df_train['Sentiment']\n",
        "X_test = df_test['Review']\n",
        "y_test = df_test['Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdmnR7GgGm9G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lT5qzpNnGm9G"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeFjvrV5Gm9G"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dYPYC1wGm9G"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert text data into numerical features\n",
        "vectorizer = CountVectorizer()\n",
        "train_vectors = vectorizer.fit_transform(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09yTGzFfGm9G",
        "outputId": "a4cf158f-a8c7-4251-9637-139a93aab89c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation scores: [0.76714801 0.8032491  0.7833935  0.78519856 0.77797834 0.77396022\n",
            " 0.78119349 0.76672694 0.78842676 0.78661844]\n",
            "Mean cross-validation score: 0.7813893367976446\n"
          ]
        }
      ],
      "source": [
        "# Train a MultinomialNB classifier on the training data with 10-fold cross-validation\n",
        "nb_clf = MultinomialNB()\n",
        "scores = cross_val_score(nb_clf, train_vectors, y_train, cv=10)\n",
        "\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "print(\"Mean cross-validation score:\", scores.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFx0AuOGGm9H"
      },
      "outputs": [],
      "source": [
        "# Train the final model on the entire training set\n",
        "nb_clf.fit(train_vectors, y_train)\n",
        "# Convert the validation and test data to vectors using the same CountVectorizer\n",
        "val_vectors = vectorizer.transform(X_val)\n",
        "test_vectors = vectorizer.transform(X_test)\n",
        "\n",
        "# Use the trained classifier to predict labels for the validation and test data\n",
        "val_predictions = nb_clf.predict(val_vectors)\n",
        "test_predictions = nb_clf.predict(test_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpbIxnoZGm9H",
        "outputId": "60e9a61e-39c2-4eb3-a2ab-74e38cc0bcdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.7174855491329479\n",
            "Test accuracy: 0.7335164835164835\n",
            "Validation recall: 0.7387140902872777\n",
            "Test recall: 0.7557755775577558\n",
            "Validation precision: 0.7297297297297297\n",
            "Test precision: 0.7231578947368421\n",
            "Validation f1 score: 0.7341944255608431\n",
            "Test f1 score: 0.7391070467993545\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on the test data\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "# Evaluate the performance of the classifier on the validation and test data\n",
        "\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "print(\"Validation accuracy:\", val_accuracy)\n",
        "print(\"Test accuracy:\", test_accuracy)\n",
        "\n",
        "val_recall = recall_score(y_val, val_predictions)\n",
        "test_recall = recall_score(y_test, test_predictions)\n",
        "print(\"Validation recall:\", val_recall)\n",
        "print(\"Test recall:\", test_recall)\n",
        "\n",
        "val_precision = precision_score(y_val, val_predictions)\n",
        "test_precision = precision_score(y_test, test_predictions)\n",
        "print(\"Validation precision:\", val_precision)\n",
        "print(\"Test precision:\", test_precision)\n",
        "\n",
        "val_f1 = f1_score(y_val, val_predictions)\n",
        "test_f1 = f1_score(y_test, test_predictions)\n",
        "print(\"Validation f1 score:\", val_f1)\n",
        "print(\"Test f1 score:\", test_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtQKgOAAGm9H",
        "outputId": "dcfef2ff-4392-4577-cb76-495b578b427c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation scores: [0.75451264 0.75812274 0.75090253 0.74909747 0.76895307 0.73960217\n",
            " 0.76853526 0.75045208 0.7721519  0.7721519 ]\n",
            "Mean cross-validation score: 0.7584481756875853\n"
          ]
        }
      ],
      "source": [
        "#SVM\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "svm_clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
        "svm_scores = cross_val_score(svm_clf, train_vectors, y_train, cv=10)\n",
        "print(\"Cross-validation scores:\", svm_scores)\n",
        "print(\"Mean cross-validation score:\", svm_scores.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwFjbLzEGm9I"
      },
      "outputs": [],
      "source": [
        "# Convert the validation and test data to vectors using the same CountVectorizer\n",
        "val_vectors = vectorizer.transform(X_val)\n",
        "test_vectors = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzBFVu94Gm9I",
        "outputId": "e98560ec-849d-4e0c-a88a-a9710965a505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.759393063583815\n",
            "Test accuracy: 0.765934065934066\n",
            "Validation recall: 0.7838577291381669\n",
            "Test recall: 0.7733773377337734\n",
            "Validation precision: 0.766042780748663\n",
            "Test precision: 0.761646803900325\n",
            "Validation f1 score: 0.7748478701825557\n",
            "Test f1 score: 0.767467248908297\n"
          ]
        }
      ],
      "source": [
        "# Train the final model on the entire training set\n",
        "svm_clf.fit(train_vectors, y_train)\n",
        "\n",
        "\n",
        "# Use the trained classifier to predict labels for the validation and test data\n",
        "val_predictions = svm_clf.predict(val_vectors)\n",
        "test_predictions = svm_clf.predict(test_vectors)\n",
        "\n",
        "# Evaluate the performance of the classifier on the validation and test data\n",
        "\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "print(\"Validation accuracy:\", val_accuracy)\n",
        "print(\"Test accuracy:\", test_accuracy)\n",
        "\n",
        "val_recall = recall_score(y_val, val_predictions)\n",
        "test_recall = recall_score(y_test, test_predictions)\n",
        "print(\"Validation recall:\", val_recall)\n",
        "print(\"Test recall:\", test_recall)\n",
        "\n",
        "val_precision = precision_score(y_val, val_predictions)\n",
        "test_precision = precision_score(y_test, test_predictions)\n",
        "print(\"Validation precision:\", val_precision)\n",
        "print(\"Test precision:\", test_precision)\n",
        "\n",
        "val_f1 = f1_score(y_val, val_predictions)\n",
        "test_f1 = f1_score(y_test, test_predictions)\n",
        "print(\"Validation f1 score:\", val_f1)\n",
        "print(\"Test f1 score:\", test_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqwCxmRvGm9I",
        "outputId": "eaa59d44-d06b-49f8-dc32-b9292908de44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation scores: [0.53116531 0.54742547 0.57091238 0.5564589  0.55826558]\n",
            "Mean cross-validation score: 0.5528455284552846\n"
          ]
        }
      ],
      "source": [
        "#KNN\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "\n",
        "#create a new KNN model\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=3)\n",
        "#train model with cv of 5 \n",
        "knn_scores = cross_val_score(knn_clf, train_vectors, y_train, cv=5)\n",
        "#print each cv score (accuracy) and average them\n",
        "\n",
        "print(\"Cross-validation scores:\", knn_scores)\n",
        "print(\"Mean cross-validation score:\", knn_scores.mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m1HQmjfGm9J",
        "outputId": "bece92bb-7967-4ee3-8a67-0e6da88cc445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.5794797687861272\n",
            "Test accuracy: 0.5758241758241758\n",
            "Validation recall: 0.5444596443228454\n",
            "Test recall: 0.5621562156215621\n",
            "Validation precision: 0.615146831530139\n",
            "Test precision: 0.5774011299435028\n",
            "Validation f1 score: 0.5776487663280117\n",
            "Test f1 score: 0.5696767001114826\n"
          ]
        }
      ],
      "source": [
        "# Train the final model on the entire training set\n",
        "knn_clf.fit(train_vectors, y_train)\n",
        "\n",
        "\n",
        "# Use the trained classifier to predict labels for the validation and test data\n",
        "val_predictions = knn_clf.predict(val_vectors)\n",
        "test_predictions = knn_clf.predict(test_vectors)\n",
        "\n",
        "# Evaluate the performance of the classifier on the validation and test data\n",
        "\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "print(\"Validation accuracy:\", val_accuracy)\n",
        "print(\"Test accuracy:\", test_accuracy)\n",
        "\n",
        "val_recall = recall_score(y_val, val_predictions)\n",
        "test_recall = recall_score(y_test, test_predictions)\n",
        "print(\"Validation recall:\", val_recall)\n",
        "print(\"Test recall:\", test_recall)\n",
        "\n",
        "val_precision = precision_score(y_val, val_predictions)\n",
        "test_precision = precision_score(y_test, test_predictions)\n",
        "print(\"Validation precision:\", val_precision)\n",
        "print(\"Test precision:\", test_precision)\n",
        "\n",
        "val_f1 = f1_score(y_val, val_predictions)\n",
        "test_f1 = f1_score(y_test, test_predictions)\n",
        "print(\"Validation f1 score:\", val_f1)\n",
        "print(\"Test f1 score:\", test_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMrzqTYOGm9J",
        "outputId": "7898d99b-bd18-4bff-a012-56801f9ff843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.6358381502890174\n",
            "Test accuracy: 0.6445054945054945\n",
            "Validation recall: 0.6757865937072504\n",
            "Test recall: 0.6578657865786579\n",
            "Validation precision: 0.6491458607095927\n",
            "Test precision: 0.6402569593147751\n",
            "Validation f1 score: 0.6621983914209115\n",
            "Test f1 score: 0.6489419424850787\n"
          ]
        }
      ],
      "source": [
        "#  Decision tree\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "# Train Decision Tree with 10-fold cross-validation\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "scores = cross_val_score(dt_clf, train_vectors, y_train, cv=10, scoring='accuracy')\n",
        "\n",
        "dt_clf.fit(train_vectors, y_train)\n",
        "# Use the trained classifier to predict labels for the validation and test data\n",
        "val_predictions = dt_clf.predict(val_vectors)\n",
        "test_predictions = dt_clf.predict(test_vectors)\n",
        "\n",
        "# Evaluate the performance of the classifier on the validation and test data\n",
        "\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "print(\"Validation accuracy:\", val_accuracy)\n",
        "print(\"Test accuracy:\", test_accuracy)\n",
        "\n",
        "val_recall = recall_score(y_val, val_predictions)\n",
        "test_recall = recall_score(y_test, test_predictions)\n",
        "print(\"Validation recall:\", val_recall)\n",
        "print(\"Test recall:\", test_recall)\n",
        "\n",
        "val_precision = precision_score(y_val, val_predictions)\n",
        "test_precision = precision_score(y_test, test_predictions)\n",
        "print(\"Validation precision:\", val_precision)\n",
        "print(\"Test precision:\", test_precision)\n",
        "\n",
        "val_f1 = f1_score(y_val, val_predictions)\n",
        "test_f1 = f1_score(y_test, test_predictions)\n",
        "print(\"Validation f1 score:\", val_f1)\n",
        "print(\"Test f1 score:\", test_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Wr74mCTGm9K",
        "outputId": "1f61d983-fd81-473b-abcf-dfa635fc6767"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.736271676300578\n",
            "Test accuracy: 0.7274725274725274\n",
            "Validation recall: 0.7783857729138167\n",
            "Test recall: 0.759075907590759\n",
            "Validation precision: 0.7370466321243523\n",
            "Test precision: 0.7135470527404343\n",
            "Validation f1 score: 0.7571523619427811\n",
            "Test f1 score: 0.7356076759061834\n"
          ]
        }
      ],
      "source": [
        "# Random Forest\n",
        "\n",
        "# Import the model we are using\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Instantiate model with 1000 decision trees\n",
        "rf_clf =RandomForestClassifier()\n",
        "# Train the model on training data\n",
        "rf_clf.fit(train_vectors, y_train)\n",
        "\n",
        "\n",
        "# Use the trained classifier to predict labels for the validation and test data\n",
        "val_predictions = rf_clf.predict(val_vectors)\n",
        "test_predictions = rf_clf.predict(test_vectors)\n",
        "\n",
        "# Evaluate the performance of the classifier on the validation and test data\n",
        "\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "print(\"Validation accuracy:\", val_accuracy)\n",
        "print(\"Test accuracy:\", test_accuracy)\n",
        "\n",
        "val_recall = recall_score(y_val, val_predictions)\n",
        "test_recall = recall_score(y_test, test_predictions)\n",
        "print(\"Validation recall:\", val_recall)\n",
        "print(\"Test recall:\", test_recall)\n",
        "\n",
        "val_precision = precision_score(y_val, val_predictions)\n",
        "test_precision = precision_score(y_test, test_predictions)\n",
        "print(\"Validation precision:\", val_precision)\n",
        "print(\"Test precision:\", test_precision)\n",
        "\n",
        "val_f1 = f1_score(y_val, val_predictions)\n",
        "test_f1 = f1_score(y_test, test_predictions)\n",
        "print(\"Validation f1 score:\", val_f1)\n",
        "print(\"Test f1 score:\", test_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tilU3gx7Gm9K",
        "outputId": "9fe08107-fb12-4cc6-ae99-90b565f9980c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting xgboost\n",
            "  Downloading xgboost-1.6.2-py3-none-win_amd64.whl (125.4 MB)\n",
            "     -------------------------------------- 125.4/125.4 MB 7.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from xgboost) (1.21.6)\n",
            "Requirement already satisfied: scipy in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from xgboost) (1.7.3)\n",
            "Installing collected packages: xgboost\n",
            "Successfully installed xgboost-1.6.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8RfQ7T0Gm9K",
        "outputId": "af6ab1db-b446-45fc-fa92-bd5564152cd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.7174855491329479\n",
            "Test accuracy: 0.7335164835164835\n",
            "Validation recall: 0.7387140902872777\n",
            "Test recall: 0.7557755775577558\n",
            "Validation precision: 0.7297297297297297\n",
            "Test precision: 0.7231578947368421\n",
            "Validation f1 score: 0.7341944255608431\n",
            "Test f1 score: 0.7391070467993545\n"
          ]
        }
      ],
      "source": [
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb_clf = XGBClassifier()\n",
        "xgb_clf.fit(train_vectors, y_train)\n",
        "\n",
        "\n",
        "# Use the trained classifier to predict labels for the validation and test data\n",
        "val_predictions = xgb_clf.predict(val_vectors)\n",
        "test_predictions = xgb_clf.predict(test_vectors)\n",
        "\n",
        "# Evaluate the performance of the classifier on the validation and test data\n",
        "\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "print(\"Validation accuracy:\", val_accuracy)\n",
        "print(\"Test accuracy:\", test_accuracy)\n",
        "\n",
        "val_recall = recall_score(y_val, val_predictions)\n",
        "test_recall = recall_score(y_test, test_predictions)\n",
        "print(\"Validation recall:\", val_recall)\n",
        "print(\"Test recall:\", test_recall)\n",
        "\n",
        "val_precision = precision_score(y_val, val_predictions)\n",
        "test_precision = precision_score(y_test, test_predictions)\n",
        "print(\"Validation precision:\", val_precision)\n",
        "print(\"Test precision:\", test_precision)\n",
        "\n",
        "val_f1 = f1_score(y_val, val_predictions)\n",
        "test_f1 = f1_score(y_test, test_predictions)\n",
        "print(\"Validation f1 score:\", val_f1)\n",
        "print(\"Test f1 score:\", test_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIZgocgqGm9L",
        "outputId": "ea73050b-0831-4ecf-bd55-be808ae61dd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "173/173 [==============================] - 1s 3ms/step - loss: 0.6971 - accuracy: 0.5050 - val_loss: 0.6946 - val_accuracy: 0.5036\n",
            "Epoch 2/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.5154 - val_loss: 0.6961 - val_accuracy: 0.5036\n",
            "Epoch 3/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.5239 - val_loss: 0.6934 - val_accuracy: 0.5036\n",
            "Epoch 4/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6926 - accuracy: 0.5227 - val_loss: 0.6935 - val_accuracy: 0.5036\n",
            "Epoch 5/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5241 - val_loss: 0.6930 - val_accuracy: 0.5036\n",
            "Epoch 6/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.5207 - val_loss: 0.6937 - val_accuracy: 0.5036\n",
            "Epoch 7/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5232 - val_loss: 0.6943 - val_accuracy: 0.5036\n",
            "Epoch 8/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6926 - accuracy: 0.5245 - val_loss: 0.6928 - val_accuracy: 0.5434\n",
            "Epoch 9/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6926 - accuracy: 0.5245 - val_loss: 0.6937 - val_accuracy: 0.5036\n",
            "Epoch 10/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5272 - val_loss: 0.6935 - val_accuracy: 0.5036\n",
            "Epoch 11/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5261 - val_loss: 0.6936 - val_accuracy: 0.5036\n",
            "Epoch 12/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6937 - val_accuracy: 0.5036\n",
            "Epoch 13/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5261 - val_loss: 0.6939 - val_accuracy: 0.5036\n",
            "Epoch 14/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5261 - val_loss: 0.6936 - val_accuracy: 0.5036\n",
            "Epoch 15/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5261 - val_loss: 0.6941 - val_accuracy: 0.5036\n",
            "Epoch 16/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6942 - val_accuracy: 0.5036\n",
            "Epoch 17/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5261 - val_loss: 0.6942 - val_accuracy: 0.5036\n",
            "Epoch 18/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6941 - val_accuracy: 0.5036\n",
            "Epoch 19/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5261 - val_loss: 0.6940 - val_accuracy: 0.5036\n",
            "Epoch 20/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6940 - val_accuracy: 0.5036\n",
            "Epoch 21/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6939 - val_accuracy: 0.5036\n",
            "Epoch 22/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6941 - val_accuracy: 0.5036\n",
            "Epoch 23/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6941 - val_accuracy: 0.5036\n",
            "Epoch 24/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6917 - accuracy: 0.5261 - val_loss: 0.6944 - val_accuracy: 0.5036\n",
            "Epoch 25/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5261 - val_loss: 0.6944 - val_accuracy: 0.5036\n",
            "Epoch 26/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5261 - val_loss: 0.6940 - val_accuracy: 0.5036\n",
            "Epoch 27/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5261 - val_loss: 0.6939 - val_accuracy: 0.5036\n",
            "Epoch 28/50\n",
            "173/173 [==============================] - 0s 3ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6940 - val_accuracy: 0.5036\n",
            "Epoch 29/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6943 - val_accuracy: 0.5036\n",
            "Epoch 30/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5261 - val_loss: 0.6942 - val_accuracy: 0.5036\n",
            "Epoch 31/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6942 - val_accuracy: 0.5036\n",
            "Epoch 32/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5261 - val_loss: 0.6942 - val_accuracy: 0.5036\n",
            "Epoch 33/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6943 - val_accuracy: 0.5036\n",
            "Epoch 34/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6940 - val_accuracy: 0.5036\n",
            "Epoch 35/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6943 - val_accuracy: 0.5036\n",
            "Epoch 36/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6941 - val_accuracy: 0.5036\n",
            "Epoch 37/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6940 - val_accuracy: 0.5036\n",
            "Epoch 38/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6939 - val_accuracy: 0.5036\n",
            "Epoch 39/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6940 - val_accuracy: 0.5036\n",
            "Epoch 40/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6939 - val_accuracy: 0.5036\n",
            "Epoch 41/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6940 - val_accuracy: 0.5036\n",
            "Epoch 42/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6942 - val_accuracy: 0.5036\n",
            "Epoch 43/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261 - val_loss: 0.6941 - val_accuracy: 0.5036\n",
            "Epoch 44/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5201 - val_loss: 0.6947 - val_accuracy: 0.5036\n",
            "Epoch 45/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5261 - val_loss: 0.6943 - val_accuracy: 0.5036\n",
            "Epoch 46/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5261 - val_loss: 0.6943 - val_accuracy: 0.5036\n",
            "Epoch 47/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5261 - val_loss: 0.6944 - val_accuracy: 0.5036\n",
            "Epoch 48/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5261 - val_loss: 0.6945 - val_accuracy: 0.5036\n",
            "Epoch 49/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5261 - val_loss: 0.6944 - val_accuracy: 0.5036\n",
            "Epoch 50/50\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.6917 - accuracy: 0.5261 - val_loss: 0.6944 - val_accuracy: 0.5036\n",
            "44/44 [==============================] - 0s 1ms/step\n",
            "Accuracy: 50.36%\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "# Preprocess the data\n",
        "corpus = df_train['Review'].tolist()\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(tokenized_corpus, vector_size=100, min_count=1)\n",
        "\n",
        "# Transform each document to a vector representation using Word2Vec\n",
        "X = []\n",
        "for doc in tokenized_corpus:\n",
        "    doc_vec = np.zeros(100)\n",
        "    count = 0\n",
        "    for word in doc:\n",
        "        try:\n",
        "            doc_vec += model.wv[word]\n",
        "            count += 1\n",
        "        except:\n",
        "            pass\n",
        "    if count != 0:\n",
        "        doc_vec /= count\n",
        "    X.append(doc_vec)\n",
        "\n",
        "X = np.array(X)\n",
        "\n",
        "# Encode the labels\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(df_train['Sentiment'])\n",
        "encoded_Y = encoder.transform(df_train['Sentiment'])\n",
        "y = np_utils.to_categorical(encoded_Y)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=100, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oO8GM5f7Gm9L",
        "outputId": "173b85f3-1e5c-4286-a15d-ac17f12513b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.10.0-cp37-cp37m-win_amd64.whl (5.0 MB)\n",
            "     ---------------------------------------- 5.0/5.0 MB 4.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow_text) (0.13.0)\n",
            "Requirement already satisfied: tensorflow<2.11,>=2.10.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow_text) (2.10.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (22.10.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (3.7.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (14.0.6)\n",
            "Requirement already satisfied: numpy>=1.20 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (21.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (3.19.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (41.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (2.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (4.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (0.27.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (1.50.0)\n",
            "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (2.10.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (2.10.0)\n",
            "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (2.10.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.11,>=2.10.0->tensorflow_text) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (2.14.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (2.28.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (2.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from packaging->tensorflow<2.11,>=2.10.0->tensorflow_text) (3.0.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (4.11.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (2022.5.18.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (2.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\renuchaurasia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (3.2.2)\n",
            "Installing collected packages: tensorflow_text\n",
            "Successfully installed tensorflow_text-2.10.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vPLcRXzsGm9L"
      },
      "outputs": [],
      "source": [
        "# BERT\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "# Preprocess the data\n",
        "corpus = df_train['Review'].tolist()\n",
        "labels = df_train['Sentiment'].tolist()\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2)\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the input text\n",
        "train_tokens = tokenizer.batch_encode_plus(\n",
        "    X_train,\n",
        "    max_length = 128,\n",
        "    padding = 'max_length',\n",
        "    truncation = True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "test_tokens = tokenizer.batch_encode_plus(\n",
        "    X_test,\n",
        "    max_length = 128,\n",
        "    padding = 'max_length',\n",
        "    truncation = True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# Convert the tokens to TensorFlow tensors\n",
        "train_tf_input = (\n",
        "    tf.constant(train_tokens['input_ids']),\n",
        "    tf.constant(train_tokens['attention_mask']),\n",
        "    tf.constant(y_train)\n",
        ")\n",
        "\n",
        "test_tf_input = (\n",
        "    tf.constant(test_tokens['input_ids']),\n",
        "    tf.constant(test_tokens['attention_mask']),\n",
        "    tf.constant(y_test)\n",
        ")\n",
        "\n",
        "# Define the BERT model\n",
        "input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32)\n",
        "attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32)\n",
        "bert_output = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask})[1]\n",
        "dropout = tf.keras.layers.Dropout(0.2)(bert_output)\n",
        "outputs = tf.keras.layers.Dense(len(set(labels)), activation='softmax')(dropout)\n",
        "model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_tf_input, epochs=10, batch_size=32,\n",
        "                    validation_data=test_tf_input)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = np.argmax(model.predict(test_tf_input), axis=-1)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nixIi1zGm9M"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text clustering\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "Apply the listed clustering methods to the dataset:\n",
        "\n",
        "K-means\n",
        "\n",
        "DBSCAN\n",
        "\n",
        "Hierarchical clustering\n",
        "\n",
        "Word2Vec\n",
        "\n",
        "BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below. \n",
        "https://www.kaggle.com/karthik3890/text-clustering "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqEdtRpbGm9M",
        "outputId": "d71284ba-8a98-4a90-8057-25fc93196d5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             Product Name Brand Name   Price  \\\n",
            "0       \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "1       \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "2       \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "3       \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "4       \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "...                                                   ...        ...     ...   \n",
            "413835  Samsung Convoy U640 Phone for Verizon Wireless...    Samsung   79.95   \n",
            "413836  Samsung Convoy U640 Phone for Verizon Wireless...    Samsung   79.95   \n",
            "413837  Samsung Convoy U640 Phone for Verizon Wireless...    Samsung   79.95   \n",
            "413838  Samsung Convoy U640 Phone for Verizon Wireless...    Samsung   79.95   \n",
            "413839  Samsung Convoy U640 Phone for Verizon Wireless...    Samsung   79.95   \n",
            "\n",
            "        Rating                                            Reviews  \\\n",
            "0            5  I feel so LUCKY to have found this used (phone...   \n",
            "1            4  nice phone, nice up grade from my pantach revu...   \n",
            "2            5                                       Very pleased   \n",
            "3            4  It works good but it goes slow sometimes but i...   \n",
            "4            4  Great phone to replace my lost phone. The only...   \n",
            "...        ...                                                ...   \n",
            "413835       5                     another great deal great price   \n",
            "413836       3                                                 Ok   \n",
            "413837       5        Passes every drop test onto porcelain tile!   \n",
            "413838       3  I returned it because it did not meet my needs...   \n",
            "413839       4  Only downside is that apparently Verizon no lo...   \n",
            "\n",
            "        Review Votes  \n",
            "0                1.0  \n",
            "1                0.0  \n",
            "2                0.0  \n",
            "3                0.0  \n",
            "4                0.0  \n",
            "...              ...  \n",
            "413835           0.0  \n",
            "413836           0.0  \n",
            "413837           0.0  \n",
            "413838           0.0  \n",
            "413839           0.0  \n",
            "\n",
            "[413840 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "#Write your code here.\n",
        "\n",
        "# read text file into pandas DataFrame\n",
        "df_amzn = pd.read_csv(\"Amazon_Unlocked_Mobile.csv\")\n",
        "\n",
        "# display DataFrame\n",
        "print(df_amzn)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6UnLooxGm9M",
        "outputId": "ad4881ec-6eb2-4ca6-e43e-41f27826ab8e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\RenuChaurasia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\seaborn\\_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:xlabel='Rating', ylabel='count'>"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEGCAYAAACpXNjrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAASUElEQVR4nO3df7Ddd13n8eerTbt0gdpgs7E2xTBuxpksaoFMqduqSMeSVqAVWZbOQEPtEmdoFWaZ1bp/WG1lQRSVgnam2tgElYJUJLjFmqkdGJBCE6j9EWCbwWKTaZvY1JaKgi1v/zifbE6Tm5sb+jnne+/N8zHznfs97++v9zn/vO73+/2c70lVIUlST8cM3YAkafExXCRJ3RkukqTuDBdJUneGiySpuyVDNzBfnHzyybVy5cqh25CkBWXbtm3/WFXLDqwbLs3KlSvZunXr0G1I0oKS5Gsz1b0sJknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqzm/oS1In73/7x4duYSIuf8+rjngbz1wkSd0ZLpKk7gwXSVJ3hoskqTvDRZLUneEiSerOcJEkdWe4SJK6M1wkSd0ZLpKk7gwXSVJ3hoskqTvDRZLUneEiSerOcJEkdWe4SJK6M1wkSd1NLFySnJbktiTbk9yb5K2t/rwkW5Lc1/4ubfUkuSbJjiR3JXnx2L7WtfXvS7JurP6SJHe3ba5JktmOIUmajkmeuTwJvL2qVgNnApclWQ1cAdxaVauAW9trgPOAVW1aD1wLo6AArgReCpwBXDkWFtcCbx7bbm2rH+oYkqQpmFi4VNWDVfWFNv914EvAqcAFwMa22kbgwjZ/AbCpRm4HTkpyCvAKYEtV7a2qR4EtwNq27MSqur2qCth0wL5mOoYkaQqmcs8lyUrgRcDngOVV9WBb9BCwvM2fCjwwttnOVputvnOGOrMcQ5I0BRMPlyTPAW4C3lZVj48va2ccNcnjz3aMJOuTbE2ydc+ePZNsQ5KOKhMNlyTHMQqWP6mqP2/lh9slLdrf3a2+CzhtbPMVrTZbfcUM9dmO8TRVdV1VramqNcuWLfvO3qQk6SCTHC0W4HrgS1X122OLNgP7RnytAz42Vr+4jRo7E3isXdq6BTg3ydJ2I/9c4Ja27PEkZ7ZjXXzAvmY6hiRpCpZMcN9nAW8E7k5yZ6v9b+BdwIeTXAp8DXhdW3YzcD6wA/gGcAlAVe1NcjVwR1vvqqra2+bfAtwAnAB8ok3McgxJ0hRMLFyq6tNADrH4nBnWL+CyQ+xrA7BhhvpW4IUz1B+Z6RiSpOnwG/qSpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHU3sXBJsiHJ7iT3jNV+NcmuJHe26fyxZb+cZEeSryR5xVh9bavtSHLFWP0FST7X6h9Kcnyr/4f2ekdbvnJS71GSNLNJnrncAKydof47VXV6m24GSLIaeD3wX9o2v5/k2CTHAr8HnAesBi5q6wL8RtvXfwYeBS5t9UuBR1v9d9p6kqQpmli4VNWngL1zXP0C4Maq+mZV/T2wAzijTTuq6qtV9S3gRuCCJAFeDnykbb8RuHBsXxvb/EeAc9r6kqQpGeKey+VJ7mqXzZa22qnAA2Pr7Gy1Q9W/G/inqnrygPrT9tWWP9bWP0iS9Um2Jtm6Z8+eZ/7OJEnA9MPlWuD7gdOBB4H3TPn4T1NV11XVmqpas2zZsiFbkaRFZarhUlUPV9VTVfVt4A8YXfYC2AWcNrbqilY7VP0R4KQkSw6oP21fbfl3tfUlSVMy1XBJcsrYy58G9o0k2wy8vo30egGwCvg8cAewqo0MO57RTf/NVVXAbcBr2/brgI+N7Wtdm38t8DdtfUnSlCw5/CrfmSQfBF4GnJxkJ3Al8LIkpwMF3A/8HEBV3Zvkw8B24Engsqp6qu3ncuAW4FhgQ1Xd2w7xS8CNSX4d+CJwfatfD3wgyQ5GAwpeP6n3KEma2cTCpaoumqF8/Qy1feu/A3jHDPWbgZtnqH+V/ZfVxuv/Cvy3I2pWktSV39CXJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd3MKlyS3zqUmSRIc5pH7SZ4F/EdGv8myFEhbdCL7f7NekqSnOdzvufwc8Dbge4Ft7A+Xx4H3T64tSdJCNmu4VNV7gfcm+fmqet+UepIkLXBz+iXKqnpfkv8KrBzfpqo2TagvSdICNqdwSfIB4PuBO4GnWrkAw0WSdJA5hQuwBlhdVTXJZiRJi8Ncv+dyD/A9k2xEkrR4zPXM5WRge5LPA9/cV6yqV0+kK0nSgjbXcPnVSTYhSVpc5jpa7JOTbkSStHjMdbTY1xmNDgM4HjgO+OeqOnFSjUmSFq65nrk8d998kgAXAGdOqilJ0sJ2xE9FrpG/AF7Rvx1J0mIw18tirxl7eQyj773860Q6kiQteHMdLfaqsfkngfsZXRqTJOkgc73ncsmkG5EkLR5z/bGwFUk+mmR3m25KsmLSzUmSFqa53tD/I2Azo991+V7g460mSdJB5houy6rqj6rqyTbdACybYF+SpAVsruHySJI3JDm2TW8AHplkY5KkhWuu4fKzwOuAh4AHgdcCb5pQT5KkBW6uQ5GvAtZV1aMASZ4H/Baj0JEk6WnmeubyQ/uCBaCq9gIvmkxLkqSFbq7hckySpftetDOXWc96kmxow5bvGd8uyZYk97W/S1s9Sa5JsiPJXUlePLbNurb+fUnWjdVfkuTuts017ZlnhzyGJGl65hou7wE+m+TqJFcDfwu8+zDb3ACsPaB2BXBrVa0Cbm2vAc4DVrVpPXAt/P8QuxJ4KXAGcOVYWFwLvHlsu7WHOYYkaUrmFC5VtQl4DfBwm15TVR84zDafAvYeUL4A2NjmNwIXjtU3tYdi3g6clOQURg/H3FJVe9tluS3A2rbsxKq6vaoK2HTAvmY6hiRpSuZ6Q5+q2g5sf4bHW15VD7b5h4Dlbf5U4IGx9Xa22mz1nTPUZzvGQZKsZ3SmxPOf//wjfS+SpEM44kfu99LOOOqwK07wGFV1XVWtqao1y5b5nVBJ6mXa4fJwu6RF+7u71XcBp42tt6LVZquvmKE+2zEkSVMy7XDZDOwb8bUO+NhY/eI2auxM4LF2aesW4NwkS9uN/HOBW9qyx5Oc2UaJXXzAvmY6hiRpSuZ8z+VIJfkg8DLg5CQ7GY36ehfw4SSXAl9j9K1/gJuB84EdwDeAS2D0fZo2Ou2Ott5V7Ts2AG9hNCLtBOATbWKWY0iSpmRi4VJVFx1i0TkzrFvAZYfYzwZgwwz1rcALZ6g/MtMxJEnTM9gNfUnS4mW4SJK6M1wkSd0ZLpKk7gwXSVJ3hoskqTvDRZLUneEiSerOcJEkdWe4SJK6M1wkSd0ZLpKk7gwXSVJ3hoskqTvDRZLUneEiSerOcJEkdWe4SJK6M1wkSd0ZLpKk7pYM3cBC8JL/tWnoFrrb9psXD92CpEXMMxdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUnd+Q1/SM/bJH/vxoVvo7sc/9cmhW1jQPHORJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKm7QcIlyf1J7k5yZ5Ktrfa8JFuS3Nf+Lm31JLkmyY4kdyV58dh+1rX170uybqz+krb/HW3bTP9dStLRa8gzl5+oqtOrak17fQVwa1WtAm5trwHOA1a1aT1wLYzCCLgSeClwBnDlvkBq67x5bLu1k387kqR95tNlsQuAjW1+I3DhWH1TjdwOnJTkFOAVwJaq2ltVjwJbgLVt2YlVdXtVFbBpbF+SpCkYKlwK+Osk25Ksb7XlVfVgm38IWN7mTwUeGNt2Z6vNVt85Q/0gSdYn2Zpk6549e57J+5EkjRnq8S9nV9WuJP8J2JLky+MLq6qS1KSbqKrrgOsA1qxZM/HjSdLRYpAzl6ra1f7uBj7K6J7Jw+2SFu3v7rb6LuC0sc1XtNps9RUz1CVJUzL1cEny7CTP3TcPnAvcA2wG9o34Wgd8rM1vBi5uo8bOBB5rl89uAc5NsrTdyD8XuKUtezzJmW2U2MVj+5IkTcEQl8WWAx9to4OXAH9aVX+V5A7gw0kuBb4GvK6tfzNwPrAD+AZwCUBV7U1yNXBHW++qqtrb5t8C3ACcAHyiTZKkKZl6uFTVV4EfnqH+CHDODPUCLjvEvjYAG2aobwVe+IyblSR9R+bTUGRJ0iJhuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndDfV7LtKCd9b7zhq6he4+8/OfGboFLRKeuUiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J1DkXVE/uGqHxy6he6e/yt3D92CtOh45iJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrpbtOGSZG2SryTZkeSKofuRpKPJogyXJMcCvwecB6wGLkqyetiuJOnosSjDBTgD2FFVX62qbwE3AhcM3JMkHTVSVUP30F2S1wJrq+p/tNdvBF5aVZcfsN56YH17+QPAV6ba6MFOBv5x4B7mCz+L/fws9vOz2G++fBbfV1XLDiwuGaKT+aKqrgOuG7qPfZJsrao1Q/cxH/hZ7OdnsZ+fxX7z/bNYrJfFdgGnjb1e0WqSpClYrOFyB7AqyQuSHA+8Htg8cE+SdNRYlJfFqurJJJcDtwDHAhuq6t6B25qLeXOJbh7ws9jPz2I/P4v95vVnsShv6EuShrVYL4tJkgZkuEiSujNc5oEkG5LsTnLP0L0MLclpSW5Lsj3JvUneOnRPQ0nyrCSfT/J37bP4taF7GlqSY5N8MclfDt3LkJLcn+TuJHcm2Tp0PzPxnss8kOTHgCeATVX1wqH7GVKSU4BTquoLSZ4LbAMurKrtA7c2dUkCPLuqnkhyHPBp4K1VdfvArQ0myf8E1gAnVtUrh+5nKEnuB9ZU1Xz4EuWMPHOZB6rqU8DeofuYD6rqwar6Qpv/OvAl4NRhuxpGjTzRXh7XpqP2v8EkK4CfAv5w6F50eIaL5q0kK4EXAZ8buJXBtMtAdwK7gS1VddR+FsDvAr8IfHvgPuaDAv46ybb2GKt5x3DRvJTkOcBNwNuq6vGh+xlKVT1VVaczesrEGUmOysumSV4J7K6qbUP3Mk+cXVUvZvTk98vapfV5xXDRvNPuL9wE/ElV/fnQ/cwHVfVPwG3A2oFbGcpZwKvbvYYbgZcn+eNhWxpOVe1qf3cDH2X0JPh5xXDRvNJuYl8PfKmqfnvofoaUZFmSk9r8CcBPAl8etKmBVNUvV9WKqlrJ6HFOf1NVbxi4rUEkeXYb7EKSZwPnAvNupKnhMg8k+SDwWeAHkuxMcunQPQ3oLOCNjP4zvbNN5w/d1EBOAW5Lchej5+VtqaqjegiuAFgOfDrJ3wGfB/5vVf3VwD0dxKHIkqTuPHORJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLNAVJnmrDqu9J8vF931+ZZf3Tx4dgJ3l1kism3qjUiUORpSlI8kRVPafNbwT+X1W9Y5b138ToqbeXT6lFqaslQzcgHYU+C/wQQJIzgPcCzwL+BbgE+HvgKuCEJGcD7wROoIVNkhuAxxk9ev57gF+sqo8kOQZ4P/By4AHg34ANVfWRKb43CfCymDRVSY4FzgE2t9KXgR+tqhcBvwL8n6r6Vpv/UFWdXlUfmmFXpwBnA68E3tVqrwFWAqsZPeXgRyb1PqTD8cxFmo4T2qPzT2X0GzVbWv27gI1JVjF6jPpxc9zfX1TVt4HtSZa32tnAn7X6Q0lu69a9dIQ8c5Gm41/ao/O/DwhwWatfDdzWfoH0VYwuj83FN8fm06tJqRfDRZqiqvoG8AvA25MsYXTmsqstftPYql8HnnuEu/8M8DNJjmlnMy97Zt1K3znDRZqyqvoicBdwEfBu4J1JvsjTL1PfBqxuw5f/+xx3fROwE9gO/DHwBeCxbo1LR8ChyNIikuQ5VfVEku9m9Dj2s6rqoaH70tHHG/rS4vKX7QuaxwNXGywaimcukqTuvOciSerOcJEkdWe4SJK6M1wkSd0ZLpKk7v4dClsTowVJS+sAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Understand how customer ratings are distributed\n",
        "import seaborn as sns\n",
        "sns.countplot(df_amzn.Rating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UtaEZi3Gm9M"
      },
      "outputs": [],
      "source": [
        "#converting the Numerical reviws to categorical reviews on codition above 3 are\n",
        "#positive and below 3 are negative as reviews rating with 3 are not much useful\n",
        "#for analysis\n",
        "\n",
        "#function\n",
        "def partition(x):\n",
        "    if x < 3:\n",
        "        return 'negative'\n",
        "    return 'positive'\n",
        "\n",
        "#changing reviews with score less than 3 to be positive\n",
        "actualScore = df_amzn['Rating']\n",
        "positiveNegative = actualScore.map(partition) \n",
        "df_amzn['Rating'] = positiveNegative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv7Lp2bIGm9M",
        "outputId": "c638bab8-e39b-4183-c75d-b1621e5d28b6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product Name</th>\n",
              "      <th>Brand Name</th>\n",
              "      <th>Price</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Review Votes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>positive</td>\n",
              "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>positive</td>\n",
              "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>positive</td>\n",
              "      <td>Very pleased</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>positive</td>\n",
              "      <td>It works good but it goes slow sometimes but i...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>positive</td>\n",
              "      <td>Great phone to replace my lost phone. The only...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        Product Name Brand Name   Price  \\\n",
              "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
              "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
              "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
              "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
              "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
              "\n",
              "     Rating                                            Reviews  Review Votes  \n",
              "0  positive  I feel so LUCKY to have found this used (phone...           1.0  \n",
              "1  positive  nice phone, nice up grade from my pantach revu...           0.0  \n",
              "2  positive                                       Very pleased           0.0  \n",
              "3  positive  It works good but it goes slow sometimes but i...           0.0  \n",
              "4  positive  Great phone to replace my lost phone. The only...           0.0  "
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_amzn.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR6ULL7mGm9N",
        "outputId": "eddb18a5-2601-44ff-d3c7-3cd15255727e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "positive    316762\n",
              "negative     97078\n",
              "Name: Rating, dtype: int64"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_amzn[\"Rating\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQMY4eKVGm9N",
        "outputId": "bbdbce4b-8078-440f-e4e6-13a3b37e6527"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(336677, 6)"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "final=df_amzn.drop_duplicates(subset={\"Product Name\",\"Rating\",\"Reviews\",\"Price\"}, keep='first', inplace=False)\n",
        "final.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_Zor4MPGm9N",
        "outputId": "c48ab281-2dcc-4469-ee10-a9cf8b506fc9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product Name</th>\n",
              "      <th>Brand Name</th>\n",
              "      <th>Price</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Review Votes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>positive</td>\n",
              "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>positive</td>\n",
              "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>positive</td>\n",
              "      <td>Very pleased</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>positive</td>\n",
              "      <td>It works good but it goes slow sometimes but i...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>positive</td>\n",
              "      <td>Great phone to replace my lost phone. The only...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        Product Name Brand Name   Price  \\\n",
              "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
              "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
              "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
              "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
              "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
              "\n",
              "     Rating                                            Reviews  Review Votes  \n",
              "0  positive  I feel so LUCKY to have found this used (phone...           1.0  \n",
              "1  positive  nice phone, nice up grade from my pantach revu...           0.0  \n",
              "2  positive                                       Very pleased           0.0  \n",
              "3  positive  It works good but it goes slow sometimes but i...           0.0  \n",
              "4  positive  Great phone to replace my lost phone. The only...           0.0  "
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_pos = final[final[\"Rating\"] == \"positive\"]\n",
        "data_neg = final[final[\"Rating\"] == \"negative\"]\n",
        "final = pd.concat([data_pos, data_neg])\n",
        "score =final[\"Rating\"]\n",
        "final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFamBk3mGm9N"
      },
      "outputs": [],
      "source": [
        "final = final.dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12OqxuOCGm9N",
        "outputId": "5481c462-bf0d-4457-c2b5-b5413bd2cd8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(278255, 62853)"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "bow = count_vect.fit_transform(final['Reviews'].values)\n",
        "bow.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFVSnXSlGm9O",
        "outputId": "934bb71e-204c-4366-a3f7-c6cdaa1a4049"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['000',\n",
              " '0000',\n",
              " '00000',\n",
              " '000000',\n",
              " '0000000',\n",
              " '00000000000',\n",
              " '0000from',\n",
              " '0001',\n",
              " '0004']"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# to understand what kind of words generated as columns by BOW\n",
        "terms = count_vect.get_feature_names()\n",
        "terms[1:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8YiOow8Gm9O",
        "outputId": "f9919a6c-108d-4184-9e08-3e636241ee99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KMeans(n_clusters=10, random_state=99)"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#using all processes jobs=-1 and k means++ for starting initilization advantage\n",
        "from sklearn.cluster import KMeans\n",
        "model = KMeans(n_clusters = 10,init='k-means++', random_state=99)\n",
        "model.fit(bow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbUM8sVhGm9O"
      },
      "outputs": [],
      "source": [
        "labels = model.labels_\n",
        "cluster_center=model.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX13rEnWGm9O",
        "outputId": "fb46ed7c-6c42-4a39-a87d-a2cd4f48e97b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[8.39858706e-04, 1.23508633e-05, 0.00000000e+00, ...,\n",
              "        6.17543166e-06, 0.00000000e+00, 6.17543166e-06],\n",
              "       [3.38242062e-02, 6.90289922e-03, 0.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [2.33791925e-02, 2.24799928e-03, 3.59679885e-04, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       ...,\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [3.00000000e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [5.99348534e-02, 7.16612378e-03, 0.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cluster_center"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfNWpObRGm9O"
      },
      "outputs": [],
      "source": [
        "i=0\n",
        "list_of_sent=[]\n",
        "for sent in final['Reviews'].values:\n",
        "    list_of_sent.append(sent.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZoTrEKNGm9O",
        "outputId": "a1b3c4ab-a6ad-4e8b-e89f-12c9e2f14454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I feel so LUCKY to have found this used (phone to us & not used hard at all), phone on line from someone who upgraded and sold this one. My Son liked his old one that finally fell apart after 2.5+ years and didn't want an upgrade!! Thank you Seller, we really appreciate it & your honesty re: said used phone.I recommend this seller very highly & would but from them again!!\n",
            "*****************************************************************\n",
            "['I', 'feel', 'so', 'LUCKY', 'to', 'have', 'found', 'this', 'used', '(phone', 'to', 'us', '&', 'not', 'used', 'hard', 'at', 'all),', 'phone', 'on', 'line', 'from', 'someone', 'who', 'upgraded', 'and', 'sold', 'this', 'one.', 'My', 'Son', 'liked', 'his', 'old', 'one', 'that', 'finally', 'fell', 'apart', 'after', '2.5+', 'years', 'and', \"didn't\", 'want', 'an', 'upgrade!!', 'Thank', 'you', 'Seller,', 'we', 'really', 'appreciate', 'it', '&', 'your', 'honesty', 're:', 'said', 'used', 'phone.I', 'recommend', 'this', 'seller', 'very', 'highly', '&', 'would', 'but', 'from', 'them', 'again!!']\n"
          ]
        }
      ],
      "source": [
        "print(final['Reviews'].values[0])\n",
        "print(\"*****************************************************************\")\n",
        "print(list_of_sent[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viYfF6G6Gm9P"
      },
      "outputs": [],
      "source": [
        "i=0\n",
        "list_of_sent_train=[]\n",
        "for sent in final['Reviews'].values:\n",
        "    filtered_sentence=[]\n",
        "    sent=cleanhtml(sent)\n",
        "    for w in sent.split():\n",
        "        for cleaned_words in cleanpunc(w).split():\n",
        "            if(cleaned_words.isalpha()):    \n",
        "                filtered_sentence.append(cleaned_words.lower())\n",
        "            else:\n",
        "                continue \n",
        "    list_of_sent_train.append(filtered_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w25kuHHyGm9P"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "# Training the wor2vec model using train dataset\n",
        "w2v_model=gensim.models.Word2Vec(list_of_sent_train, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWyJcggvGm9P",
        "outputId": "ce7fb085-e62d-41bd-d3d1-6cc909091d14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\RenuChaurasia\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(278255, 100)"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this train\n",
        "for sent in list_of_sent_train: # for each review/sentence\n",
        "    sent_vec = np.zeros(100) # as word vectors are of zero length\n",
        "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
        "    for word in sent: # for each word in a review/sentence\n",
        "        try:\n",
        "            vec = w2v_model.wv[word]\n",
        "            sent_vec += vec\n",
        "            cnt_words += 1\n",
        "        except:\n",
        "            pass\n",
        "    sent_vec /= cnt_words\n",
        "    sent_vectors.append(sent_vec)\n",
        "sent_vectors = np.array(sent_vectors)\n",
        "sent_vectors = np.nan_to_num(sent_vectors)\n",
        "sent_vectors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwXwgRImGm9P",
        "outputId": "05aab25c-dac6-47bf-924e-eb6b47c33d80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[3, 4, 5, 6, 7, 8, 9, 10]"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Number of clusters to check.\n",
        "num_clus = [x for x in range(3,11)]\n",
        "num_clus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdfjAlVgGm9P"
      },
      "outputs": [],
      "source": [
        "squared_errors = []\n",
        "for cluster in num_clus:\n",
        "    kmeans = KMeans(n_clusters = cluster).fit(sent_vectors) # Train Cluster\n",
        "    squared_errors.append(kmeans.inertia_) # Appending the squared loss obtained in the list\n",
        "    \n",
        "optimal_clusters = np.argmin(squared_errors) + 2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx4GQnYgGm9Q",
        "outputId": "3aefc2a1-fbdd-4a91-e7de-38f1b6710554"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KMeans(n_clusters=9)"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training the best model --\n",
        "from sklearn.cluster import KMeans\n",
        "model2 = KMeans(n_clusters = optimal_clusters)\n",
        "model2.fit(sent_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vfqa0gY4Gm9Q"
      },
      "outputs": [],
      "source": [
        "word_cluster_pred=model2.predict(sent_vectors)\n",
        "word_cluster_pred_2=model2.labels_\n",
        "word_cluster_center=model2.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r6XdYocGm9Q",
        "outputId": "cf28c3aa-78e6-4672-b428-fe3ad216aa15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.5706813 , -0.49902229,  0.05172537, -0.29230335, -0.27650186,\n",
              "         0.16437484, -0.44766987, -0.27226919,  0.11549864,  0.24196486,\n",
              "         0.19906273,  0.36722626, -0.27633084, -0.29177358, -0.32519292,\n",
              "         0.11995747, -0.05667634,  1.04522652,  0.63649435,  0.44707405,\n",
              "        -0.16407088, -0.08538789, -0.26048373,  0.71929574,  0.44749549,\n",
              "        -0.28665678,  0.7132712 ,  0.28665266,  0.30381644,  0.13161832,\n",
              "        -0.20834765, -0.34541565,  0.18868703,  0.65218866, -0.23441523,\n",
              "         0.13472996,  0.2683659 , -0.07446211,  0.24368691,  0.58708295,\n",
              "         0.22752427, -0.02028882, -0.03079049,  0.17582125, -0.37734302,\n",
              "         0.07683343,  0.13139973, -0.1460146 ,  0.12568788, -0.62082482,\n",
              "         0.53693111, -0.14836564,  0.31280875, -0.25203315, -0.14461011,\n",
              "         0.03707706, -0.37312035, -0.44971635,  0.0555543 , -0.12987089,\n",
              "        -0.24809946,  0.39754658,  0.34319375,  0.61046568, -0.15491831,\n",
              "        -0.68954717, -0.15035784,  0.41555939, -0.31859658,  0.05511539,\n",
              "        -0.03318309, -0.16775404,  0.19602913,  0.17691886, -0.16798467,\n",
              "         0.33387071,  0.14041305,  0.10607417, -0.3661083 , -0.26249177,\n",
              "        -0.26450218, -0.0834899 ,  0.40424758, -0.48402488,  0.01333871,\n",
              "        -0.1029263 , -0.22402203, -0.24182067,  0.29376702,  0.51532168,\n",
              "         0.22159498,  0.04180324, -0.51980628, -0.27967176,  0.19933161,\n",
              "        -0.60644173,  0.48439021, -0.28947628,  0.74737919, -0.01767652]])"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_cluster_center[1:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaUOclRKGm9Q"
      },
      "source": [
        "In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nYNxV_CGm9Q"
      },
      "source": [
        "K-means, DBSCAN, and hierarchical clustering are unsupervised clustering algorithms that are commonly used for grouping similar data points together based on their feature similarities. K-means is a centroid-based algorithm that aims to minimize the variance of clusters by assigning each data point to the nearest centroid. DBSCAN is a density-based algorithm that groups together points that are closely packed together and considers points that are isolated as noise. Hierarchical clustering is a bottom-up or top-down approach that recursively merges or splits clusters based on a linkage criteria such as distance or similarity.\n",
        "\n",
        "In contrast, Word2Vec and BERT are language models used for natural language processing tasks such as text classification, sentiment analysis, and machine translation. Word2Vec is a neural network-based approach that learns vector representations of words in a corpus by predicting the context words given a target word. BERT, on the other hand, is a transformer-based model that uses a bidirectional encoding scheme to generate contextualized word embeddings.\n",
        "\n",
        "The results of these algorithms depend on the specific problem and the nature of the data being analyzed. K-means and hierarchical clustering are often used when the number of clusters is known a priori, while DBSCAN can be useful when the data contains outliers or has varying densities. Word2Vec and BERT are powerful language models that can learn complex semantic relationships between words and are often used for downstream natural language processing tasks. Overall, the choice of algorithm depends on the specific problem and the characteristics of the data being analyzed."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}