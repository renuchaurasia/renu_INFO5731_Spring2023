{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renuchaurasia/renu_INFO5731_Spring2023/blob/main/In_class_exercise/Copy_of_In_class_exercise_02_02072023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecu7X7MzzmWQ"
      },
      "source": [
        "## The second In-class-exercise (02/07/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXqpOEfUzmWR"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CafDz5NAzmWR"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2i9XFIrzmWR"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "\n",
        "Why do people write reviews\n",
        "\n",
        "\n",
        "Reviews provide simple, relevant guidance in a world filled with choices, such as Which series should I watch next on Amazon Prime? Which insurance provider is best for me and my family? Decisions, even minor ones, can be difficult and tiresome, particularly when we’re presented with so many options. Humans can only process so much information at once.\n",
        "\n",
        "\n",
        "consumers read reviews because reviews:\n",
        "People who have used the product or service can provide valuable guidance\n",
        "Make it easier to decide\n",
        "Attest to a company’s reliability\n",
        "Describe what could go wrong and what could go right\n",
        "Provide insight into the quality of products to consumers\n",
        "Consumers read reviews because they trust them and believe they will help them make the right purchases or choose the right company.\n",
        "\n",
        "We can collect product reviews on the marketplace such as Amazon.com, walmart, flipkart, etc. Here consumers buy the product and write comments about the product and this helps many other consumers to know more about the product quality, effect of the product and the other information which are not mentioned in the website about the product. \n",
        "We can collect at least 4k-6k data about the same type of product to decide the quality of the product\n",
        "\n",
        "At this time I am collecting the customer review of the product for that I am scrapping the walmart customer product review.\n",
        "For this web scraping I will be using selenium, pandas and request python modules. Create a url list variable which will have the collection of walmart product urls. Then will get the response and store this in soup using BeautifulSoup(). From this soup I will be collecting product name, reviewer name, reviewing date, heading, review text and starts given to the product.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0m2gDAqzmWS"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WrqG5u0zmWT"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# Product page URL (Put your URL)\n",
        "url_list = [\"https://www.walmart.com/ip/Melrose-Ave-Women-s-Faux-Leather-Crossband-Slide-Sandals/582376302\",\n",
        "            \"https://www.walmart.com/ip/Melrose-Ave-Women-s-Faux-Leather-Crossband-Slide-Sandals/582376302\",\n",
        "\"https://www.walmart.com/ip/Steve-Madden-Women-s-Balanced-Slide-Sandal/869287599\",\n",
        "\"https://www.walmart.com/ip/ELOQUII-Elements-Women-s-Plus-Size-Floral-Print-Bubble-Top/407198443\",\n",
        "\"https://www.walmart.com/ip/ELOQUII-Elements-Women-s-Plus-Size-Belted-Denim-Utility-Dress/331688468\",\n",
        "\"https://www.walmart.com/ip/The-Get-Women-s-Plus-Size-Long-Sleeve-Button-Front-Blouse-with-Ruffles/223296994\",\n",
        "\"https://www.walmart.com/ip/The-Get-Women-s-Size-Plus-Long-Sleeve-Ruffle-Sweatshirt/859174758\",\n",
        "\"https://www.walmart.com/ip/Sofia-Jeans-by-Sofia-Vergara-Women-s-Plus-Size-Melisa-Flare-Jeans-with-Side-Piecing/110887406\",\n",
        "\"https://www.walmart.com/ip/Great-Value-Whole-Vitamin-D-Milk-Gallon-128-fl-oz/10450114\",\n",
        "\"https://www.walmart.com/ip/Scoop-Women-s-Long-Sleeve-Ruffle-Mini-Dress/593737384\",\n",
        "\"https://www.walmart.com/ip/TRESemm-Tres-Two-Frizz-Control-Humidity-Resistant-Freeze-Hold-Hair-Spray-11-oz/15580724\",\n",
        "\"https://www.walmart.com/ip/The-Get-Women-s-Plus-Size-Short-Sleeve-Midi-Dress-with-Puff-Shoulders/683339551\",\n",
        "\"https://www.walmart.com/ip/Beats-by-Dr-Dre-Bluetooth-Sports-In-Ear-Headphones-with-Built-in-Microphone-Flame-Blue-MYMG2LL-A/215296930\",\n",
        "\"https://www.walmart.com/ip/Time-and-Tru-Women-s-Braided-Two-Band-Sandals/368007817\",\n",
        "\"https://www.walmart.com/ip/Great-Value-Whole-Vitamin-D-Milk-Gallon-128-fl-oz/10450114\",\n",
        "\"https://www.walmart.com/ip/Skittles-Starburst-Assorted-Easter-Chewy-Candy-Variety-Pack-20-39-oz-50-Piece-Bag/296096177\",\n",
        "\"https://www.walmart.com/ip/Hormel-Gatherings-Pepperoni-and-Cheese-Snack-Tray-14-oz/10535982\",\n",
        "\"https://www.walmart.com/ip/Hormel-Gatherings-Pepperoni-and-Cheese-Snack-Tray-14-oz/10535982\",\n",
        "\"https://www.walmart.com/ip/REESE-S-Milk-Chocolate-Peanut-Butter-Eggs-Candy-Easter-1-2-oz-Packs-6-Count/20077380\",\n",
        "\"https://www.walmart.com/ip/CADBURY-Mini-Eggs-Milk-Chocolate-with-Crisp-Shell-Candy-Easter-10-oz-Bag/20077407\",\n",
        "\"https://www.walmart.com/ip/Great-Value-Whole-Vitamin-D-Milk-Gallon-128-fl-oz/10450114\"\n",
        "\"https://www.walmart.com/ip/M-M-s-Easter-Minis-Milk-Chocolate-Candy-1-77-oz-Tube/55334945\",\n",
        "\"https://www.walmart.com/ip/CADBURY-Milk-Chocolate-Cr-me-Egg-Candy-Easter-1-2-oz-Pack-5-Count/20077360\",\n",
        "\"https://www.walmart.com/ip/REESE-S-Milk-Chocolate-Peanut-Butter-Eggs-Candy-Easter-1-2-oz-Pack-36-ct/451851338\",\n",
        "\"https://www.walmart.com/ip/REESE-S-Milk-Chocolate-Peanut-Butter-Eggs-Candy-Easter-1-2-oz-Packs-6-Count/20077380\",\n",
        "\"https://www.walmart.com/ip/CADBURY-Milk-Chocolate-with-Caramel-Center-Mini-Caramel-Eggs-Candy-Easter-3-8-oz-Carton-12-Pieces/20077357\",\n",
        "\"https://www.walmart.com/ip/CADBURY-CREME-EGG-Milk-Chocolate-with-a-Soft-Fondant-Center-Mini-Eggs-Candy-Easter-3-8-oz-Carton-12-Pieces/20077379\",\n",
        "\"https://www.walmart.com/ip/REESE-S-Milk-Chocolate-Peanut-Butter-Eggs-Candy-Easter-1-2-oz-Packs-6-Count/20077380\",\n",
        "\"https://www.walmart.com/ip/REESE-S-Miniatures-Milk-Chocolate-Peanut-Butter-Cups-Candy-Easter-35-6-oz-Bulk-Party-Bag/610414849\",\n",
        "\"https://www.walmart.com/ip/HERSHEY-S-KISSES-Milk-Chocolate-Candy-Easter-35-8-oz-Party-Bag/944653593\"\n",
        "\"https://www.walmart.com/ip/REESE-S-Milk-Chocolate-Peanut-Butter-Eggs-Candy-Easter-1-2-oz-Packs-6-Count/20077380\",\n",
        "\"https://www.walmart.com/ip/REESE-S-Milk-Chocolate-Peanut-Butter-Eggs-Candy-Easter-1-2-oz-Pack-36-ct/451851338\",\n",
        "\"https://www.walmart.com/ip/REESE-S-Milk-Chocolate-Peanut-Butter-Cups-Candy-Individually-Wrapped-9-oz-Pack-6-Pack/10449927\",\n",
        "\"https://www.walmart.com/ip/REESE-S-Milk-Chocolate-Peanut-Butter-Eggs-Candy-Easter-1-2-oz-Pack-36-ct/451851338\",\n",
        "\"https://www.walmart.com/ip/PEEPS-Yellow-Marshmallow-Chicks-Easter-Candy-3oz-10ct/19698957\"\n",
        "\"https://www.walmart.com/ip/Peeps-Blue-Marshmallow-Chicks-Easter-Candy-3-Oz/19698956\",\n",
        "\"https://www.walmart.com/ip/PEEPS-Cotton-Candy-Flavored-Marshmallow-Chicks-Easter-Candy-10ct-3-0-oz/179649516\",\n",
        "\"https://www.walmart.com/ip/M-M-s-Easter-Minis-Milk-Chocolate-Candy-1-77-oz-Tube/55334945?athbdg=L1600\"\n",
        "]\n",
        "reviews =[]\n",
        "for url in url_list:\n",
        "    # Get Product Id\n",
        "    product_id = url[url.rfind(\"/\")+1::]\n",
        "    int_url = url[0:url.rfind(\"/\")]\n",
        "    product_name = int_url[int_url.rfind(\"/\")+1::].replace(\"-\",\" \")\n",
        "    header={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36','referer':'https://www.amazon.com/s?k=nike+shoes+men&crid=28WRS5SFLWWZ6&sprefix=nike%2Caps%2C357&ref=nb_sb_ss_organic-diversity_2_4'}\n",
        "\n",
        "    response = requests.get(\"https://www.walmart.com/reviews/product/\"+product_id, headers=header)\n",
        "    soup=BeautifulSoup(response.content)\n",
        "    for i in soup.findAll(\"li\",{'class':\"dib w-100 mb3\"}):\n",
        "        review = {}\n",
        "        stars = i.findAll('span', {'class':\"w_DF\"})\n",
        "        heading = i.findAll('span', {'class':\"b w_O\"})\n",
        "        review_text = i.findAll('span', {'class':\"tl-m db-m\"})\n",
        "        reviewer = i.findAll('div', {'class':\"f6 gray pr2\"})\n",
        "        review_date = i.findAll('div', {'class':\"f7 gray\"})\n",
        "\n",
        "        review[\"product_name\"] = product_name\n",
        "        review[\"reviewer\"] = reviewer[0].text if len(reviewer)>0 else \"\"\n",
        "        review[\"review_date\"] = review_date[0].text\n",
        "        review[\"heading\"] = heading[0].text\n",
        "        review[\"review_text\"] = review_text[0].text\n",
        "        review[\"stars\"] = stars[0].text\n",
        "\n",
        "        reviews.append(review)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDTmdhM6zmWT"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2012-2022).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from selenium import webdriver\n",
        "import pandas as pd\n",
        "import re\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "\n",
        "\n",
        "def sigir_proceeding(base_url, proceeding_ids):\n",
        "    print(1111)\n",
        "    \n",
        "    print(1)\n",
        "    # comments = pd.DataFrame(columns=['article_type', 'article_title', 'article_author', 'article_year','article_id','article_abstract','total_citation','total_download','pdf_link'])\n",
        "    for proceeding_id in proceeding_ids:\n",
        "        print(proceeding_id)\n",
        "        comments = pd.DataFrame(\n",
        "            columns=['article_type', 'article_title', 'article_author', 'article_year', 'article_id',\n",
        "                     'article_abstract', 'total_citation', 'total_download', 'pdf_link'])\n",
        "        proceeding_year = proceeding_id[0]\n",
        "        proceeding_url = base_url + proceeding_id[1]\n",
        "        # print(proceeding_url)\n",
        "        driver = webdriver.Chrome(r'chromedriver.exe')\n",
        "        driver.get(proceeding_url)\n",
        "        sessions = driver.find_element_by_xpath(\"/html/body/div[1]/div/main/div[4]/div/div[2]/div[1]/div[1]/div[2]/div/div\")\n",
        "        all_sessions = sessions.text.split('\\n')\n",
        "        print(len(all_sessions))\n",
        "        for i in range(len(all_sessions)):\n",
        "            session_url = proceeding_url+\"?tocHeading=heading\"+str(i+1)\n",
        "            # print(session_url)\n",
        "            driver.get(session_url)\n",
        "            # get the article list under each session\n",
        "            articles = driver.find_elements_by_class_name(\"issue-item-container\")\n",
        "            for article in articles:\n",
        "                # article type information\n",
        "                article_type = article.find_element_by_class_name(\"issue-heading\").text\n",
        "                print(article_type)\n",
        "                if article_type == 'SECTION':\n",
        "                    continue\n",
        "                else:\n",
        "                    # article meta-information\n",
        "                    article_information = article.find_element_by_class_name(\"issue-item__content\").text.split('\\n')\n",
        "                    print(article_information)\n",
        "                    # article title\n",
        "                    article_title = article_information[0]\n",
        "                    # article author\n",
        "                    article_author = article_information[1]\n",
        "                    # article year\n",
        "                    if len(re.findall(r'2\\d\\d\\d', article_information[2])) > 0:\n",
        "                        article_year = re.findall(r'2\\d\\d\\d', article_information[2])[0]\n",
        "                    else:\n",
        "                        article_year = ''\n",
        "                    # print(article_year)\n",
        "\n",
        "                    # article doi\n",
        "                    if len(re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
        "                                      article_information[2])) > 0:\n",
        "                        article_url = \\\n",
        "                            re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
        "                                       article_information[2])[0]\n",
        "                    else:\n",
        "                        article_url = ''\n",
        "                    # print(article_url)\n",
        "                    # abstract\n",
        "                    if len(article_information)>3:\n",
        "                        article_abstract = article_information[3]\n",
        "                    else:\n",
        "                        article_abstract = ''\n",
        "                    # citation count\n",
        "                    if len(article_information)>4:\n",
        "                        total_citation = article_information[4]\n",
        "                    else:\n",
        "                        total_citation = ''\n",
        "                    # download count\n",
        "                    if len(article_information)>5:\n",
        "                        total_download = article_information[5]\n",
        "                    else:\n",
        "                        total_download = ''\n",
        "                    # download_link\n",
        "                    try:\n",
        "                        pdf_link = article.find_element_by_class_name(\"btn--icon.simple-tooltip__block--b.red.btn\")\n",
        "                        if pdf_link != None:\n",
        "                            download_link = pdf_link.get_attribute('href')\n",
        "                            print(download_link)\n",
        "                        else:\n",
        "                            download_link = ''\n",
        "                    except NoSuchElementException:\n",
        "                        download_link = ''\n",
        "                    comments.loc[len(comments)] = [article_type, article_title, article_author,article_year,article_url,article_abstract,total_citation,total_download,download_link]\n",
        "        print(comments)\n",
        "        comments.to_csv(str(proceeding_year)+'_sigir_article_information.csv')\n",
        "\n",
        "\n",
        "\n",
        "    # proceeding_ids = [[2020,'10.1145/3397271']]\n",
        "proceeding_ids = [[2002,'10.1145/564376'],[2003,'10.1145/860435'],[2004,'10.1145/1008992'],[2005,'10.1145/1076034'], \n",
        "                      [2006,'10.1145/1148170'],[2007,'10.1145/1277741'], [2008,'10.1145/1390334'],[2009,'10.1145/1571941'],\n",
        "                      [2010,'10.1145/1835449'],[2011,'10.1145/2009916'],[2012,'10.1145/2348283'],[2013,'10.1145/2484028'],\n",
        "                      [2014,'10.1145/2600428'],[2015,'10.1145/2766462'],[2016,'10.1145/2911451'],[2017,'10.1145/3077136'],\n",
        "                      [2018,'10.1145/3209978'],[2019,'10.1145/3331184'],[2020,'10.1145/3397271']]\n",
        "base_url = 'https://dl.acm.org/doi/proceedings/'\n",
        "sigir_proceeding(base_url,proceeding_ids)"
      ],
      "metadata": {
        "id": "BmlwcZ93lmmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eBcgijnzmWT"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPhHjoT-zmWV"
      },
      "outputs": [],
      "source": [
        "import tweepy           #Module used to connect twitter from python\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "\n",
        "\n",
        "#Check the above URL on how to create twitter developer account and project in developer account.\n",
        "api_key = 'm8mXqjFwSH17RhCzB1TKE2rpm'\n",
        "api_key_secret = 'lgdGHMPW0tuJORHah9pgn31g1u3iTpzRJiUIiOP94M815FL2fM'\n",
        "access_token = '1574153004256825344-wgBprx4NNAPh5XdGa36bVpZhhqoAXv'\n",
        "access_token_secret = '9Flz6GKgfl2Ot53lG2cb7AoUxKZ05ZCcRqDj1dQC3uJTO'\n",
        "\n",
        "\n",
        "auth = tweepy.OAuthHandler(api_key, api_key_secret)  #OauthHandler is the authetication function from tweepy module to autheticate the twitter developer users to acees the twitter data.\n",
        "auth.set_access_token(access_token, access_token_secret)   #we are storing the access tokens so that we dont need to fetch the secrets everytime the authetication expires.\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True) #Connecteing to twitter API using the authetication in the previous steps using the API fucntion of tweepy module.\n",
        "\n",
        "\n",
        "#This fucntion will loop through the paginations in the twitter api results.\n",
        "\n",
        "cursor = tweepy.Cursor(api.search, q=\"BBC News\", tweet_mode = 'extended').items(1000)\n",
        "\n",
        "# Just creating the nested lists here to make to mode tabluar or structure format for further analysis.\n",
        "tweets = [[i. created_at, i.user.screen_name, i.full_text] for i in cursor]\n",
        "\n",
        "# Created the pandas dataframe using the nested lists generated above.\n",
        "tweetsDf = pd.DataFrame(data=tweets,columns=['Username','Time','Tweet'])\n",
        "tweetsDf.to_csv('BBC_NEWS.csv')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "c5f8f379d90e93d41b0750002e16acd665ca35ef2d477cfa8d1f4ac9754ac95e"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
